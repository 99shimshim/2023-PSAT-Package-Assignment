\[시계열자료분석팀\] 심현구 클린업 3주차 패키지과제
================
심현구
2023-09-26

``` r
# setwd
setwd("C:/Users/mrg07/Desktop/99shimshim/SKKU/PSAT/2023_2/클린업/week3/[23년도 2학기]클린업 3주차 패키지")

# packages needed

library(tidyverse)
library(gridExtra)
library(ggcorrplot)
```

# Chapter 1 : EDA

#### 문제1. Train, test 데이터를 불러온 뒤 데이터의 구조를 파악하세요.

``` r
train <-  read.csv("train.csv")
test <- read.csv("test.csv")

print(head(train, n = 1))
```

    ##   X longitude latitude housingMedianAge totalRooms totalBedrooms population
    ## 1 2   -122.22    37.86               21       7099          1106       2401
    ##   households medianIncome medianHouseValue
    ## 1       1138       8.3014           358500

``` r
print(str(train))
```

    ## 'data.frame':    16640 obs. of  10 variables:
    ##  $ X               : int  2 3 4 5 8 9 10 11 12 13 ...
    ##  $ longitude       : num  -122 -122 -122 -122 -122 ...
    ##  $ latitude        : num  37.9 37.9 37.9 37.9 37.8 ...
    ##  $ housingMedianAge: int  21 52 52 52 52 42 52 52 52 52 ...
    ##  $ totalRooms      : int  7099 1467 1274 1627 3104 2555 3549 2202 3503 2491 ...
    ##  $ totalBedrooms   : int  1106 190 235 280 687 665 707 434 752 474 ...
    ##  $ population      : int  2401 496 558 565 1157 1206 1551 910 1504 1098 ...
    ##  $ households      : int  1138 177 219 259 647 595 714 402 734 468 ...
    ##  $ medianIncome    : num  8.3 7.26 5.64 3.85 3.12 ...
    ##  $ medianHouseValue: num  358500 352100 341300 342200 241400 ...
    ## NULL

``` r
# 불필요한 index X 제거
train <- train %>% 
  select(-X)

test <- test %>% 
  select(-X)
```

#### 문제2. Train과 Test 데이터에서 결측치가 존재하는지 확인해주세요.

``` r
print(sum(is.na(train)))
```

    ## [1] 0

``` r
print(sum(is.na(test)))
```

    ## [1] 0

``` r
# 결측치 없음
```

#### 문제3. 데이터 기반의 모델링을 하기 위해서는 독립변수의 분포를 확인하는 것이 필수적입니다. 아래와 같이 분포를 시각화해주세요.

``` r
p1 <- ggplot(data = train) +
  geom_histogram(mapping = aes(x = medianHouseValue), color = "darkblue", fill = "lightblue") +
  theme_classic()

p2 <- ggplot(data = train) +
  geom_point(mapping = aes(x = longitude, y = latitude, color = latitude+longitude), show.legend = FALSE) +
  scale_color_gradient(low = "black", high = "white") +
  theme_classic()

# 두 변수를 동시에 고려해서 색을 입히고 싶으므로 더해주었음


p3 <- ggplot(data = train) +
  geom_histogram(mapping = aes(x = housingMedianAge), color = "darkblue", fill = "lightblue") +
  theme_classic()

p4 <- ggplot(data = train) +
  geom_histogram(mapping = aes(x = totalRooms), color = "darkblue", fill = "lightblue") +
  theme_classic()

p5 <- ggplot(data = train) +
  geom_histogram(mapping = aes(x = totalBedrooms), color = "darkblue", fill = "lightblue") +
  theme_classic()

p6 <- ggplot(data = train) +
  geom_histogram(mapping = aes(x = population), color = "darkblue", fill = "lightblue") +
  theme_classic()

p7 <- ggplot(data = train) +
  geom_histogram(mapping = aes(x = households), color = "darkblue", fill = "lightblue") +
  theme_classic()

p8 <- ggplot(data = train) +
  geom_histogram(mapping = aes(x = medianIncome), color = "darkblue", fill = "lightblue") +
  theme_classic()

grid.arrange(p1,p2,p3,p4,p5,p6,p7,p8, nrow = 2)
```

![](%5B시계열자료분석팀%5D-심현구-클린업-3주차-패키지과제_files/figure-gfm/unnamed-chunk-4-1.png)<!-- -->

#### 문제4. 각 독립변수들의 관계가 존재하는지 확인하기 위해서 상관관계를 확인해보도록 하겠습니다. 아래와 같이 상관관계를 시각화 해주세요.

``` r
cormat_train <- cor(train)

ggcorrplot(cormat_train, lab = T, lab_size = 2, tl.srt = 45, tl.cex = 10, title = "Correlation of CaliforniaHousing")
```

![](%5B시계열자료분석팀%5D-심현구-클린업-3주차-패키지과제_files/figure-gfm/unnamed-chunk-5-1.png)<!-- -->

#### 문제5. 종속변수(medianHouseValue)의 경우 큰 값이 극단적으로 많습니다. medianHouseValue가 3분위수 이상인 경우 1을 가지고 그렇지 않다면 0을 가지는 rich라는 변수를 만들어주세요.

``` r
train <- train %>%
  mutate(rich = as.factor(ifelse(medianHouseValue >= quantile(medianHouseValue, 0.75), 1, 0)))
```

#### 문제6. 위도와 경도에 따른 변수의 변화를 보기 위해 다음과 같이 시각화를 진행해주세요. 왼쪽의 경우 rich를 이용해 1인 경우와 아닌 경우로 scatterplot을 오른쪽의 경우는 medianHouseValue를 이용해 시각화한 것입니다.

``` r
q1 <- ggplot(data = train) +
  geom_point(mapping = aes(x = longitude, y = latitude, color = rich))

q2 <- ggplot(data = train) +
  geom_point(mapping = aes(x = longitude, y = latitude, color = medianHouseValue)) +
   scale_color_gradient(low = "black", high = "yellow")

grid.arrange(q1,q2, nrow = 1)
```

![](%5B시계열자료분석팀%5D-심현구-클린업-3주차-패키지과제_files/figure-gfm/unnamed-chunk-7-1.png)<!-- -->
남서쪽에 위치한 집들이 북동쪽에 위치한 집에 비해 비싼 경향이 나타난다.
특히, rich로 나타낸 산점도를 보면 3사분위수 이상의 비싼 집들이 남서쪽에
집중적으로 분포하는 모습을 확인할 수 있다.

# Chapter 2: KNN Regression

#### 문제1. 항상 동일한 결과를 얻기 위하여 Seed를 고정해주세요 (Seed: 3132)

``` r
set.seed(3132)
```

#### 문제2. KNN이 어떤 모델인지 살펴보고, 하이퍼 파라미터인 K개 커질 때 모델의 bias와 variance는 어떻게 변하는 지 간단하게 설명해주세요.

K-Nearest Neighbors (KNN)는 지도 학습 알고리즘 중 하나로, 주어진 데이터
포인트 주변에 가장 가까운 K개의 이웃을 찾아 이들의 다수결 또는 평균을
사용하여 분류 또는 회귀 작업을 수행하는 알고리즘입니다. KNN은
간단하면서도 강력한 알고리즘 중 하나이며, 이해하기 쉽고 직관적입니다.

K가 클 때:

- 큰 K 값은 모델을 더 간단하게 만듭니다.
- 이웃의 수가 많기 때문에 모델은 노이즈에 덜 민감하며 부드럽고 안정적인
  예측을 합니다.
- 큰 K 값은 훈련 데이터에 대한 예측이 더 안정적이지만, 모델이 데이터의
  세부 사항을 놓치기 쉬울 수 있습니다.
- 큰 K 값은 모델의 분산이 감소하고 편향이 증가합니다.

#### 문제3. Caret 패키지를 불러와주세요.

``` r
library(caret)
```

#### 문제4. Caret 패키지의 train 함수를 이용하여 하이퍼파라미터 튜닝과 학습을 동시에 진행하겠습니다.

- expand.grid(k = 1:10)을 통해 최적의 k를 찾을 범위를 설정해주세요.
- trainControl을 통해 5번의 교차검증을 하도록 설정해주세요.
- 학습에는 longitude와 latitude만을 이용해서 종속변수를 예측해주세요.

``` r
param_grid <- expand.grid(k = 1:10)

ctrl <- caret::trainControl(method = "cv", number = 5)

knn_model <- train(medianHouseValue ~ longitude + latitude,
                   data = train,
                   method = "knn",
                   trControl = ctrl,
                   tuneGrid = param_grid)
```

#### 문제5. 가장 낮은 RMSE를 갖는 K를 이용하여 train 데이터의 예측값을 y_hat_knn으로 저장하고 아래와 같은 residual plot을 그려주세요.

``` r
# 가장 낮은 RMSE를 갖는 K 선택
best_k <- knn_model$bestTune$k

# KNN 모델 학습 (가장 낮은 RMSE를 갖는 K로)
final_knn_model <- caret::train(medianHouseValue ~ longitude + latitude, 
                                data = train, 
                                method = "knn", 
                                trControl = ctrl, 
                                tuneGrid = data.frame(k = best_k))

# train 데이터에 대한 예측
y_hat_knn <- predict(final_knn_model, train)

residuals <- train$medianHouseValue - y_hat_knn     # residual 계산

# Residual plot 그리기
plot(y_hat_knn, residuals, main = "Residual Plot for KNN Model",
     xlab = "y_hat", ylab = "res_knn")
abline(h = 0, col = "red")  # Residual이 0인 수평선 추가
```

![](%5B시계열자료분석팀%5D-심현구-클린업-3주차-패키지과제_files/figure-gfm/unnamed-chunk-11-1.png)<!-- -->

#### (+ 보너스 문제 1) 위 residual plot을 해석해주세요

잔차의 평균은 0에 가까워 보이나, `mean(residuals)`를 통해 확인해보면
2762.678로, 0과는 거리가 있음을 보인다. 이를 통해 모델이 잔차를 약간
과소예측하고 있음을 알 수 있다.

또한, 데이터포인트들이 빨간 선을 기준으로 무작위로 분포해야 이상적이라고
할 수 있는데, 그렇다고 보기는 어렵다. 좌측 중앙에 약간 치우친 분포를
보인다.

그래프에서 어떤 패턴도 확인되지 않아야 이상적이라고 할 수 있는데, 우측
상단에 대각선 모양으로 잔차들이 패턴을 이루고 있음이 확인된다.

약간의 치우친 분포, 패턴이 확인되고 평균도 0에 가깝다고 보기는 어려워서
이상적인 모델이라고는 할 수 없다. 이는 모델 학습 시 독립변수로 공간적인
요소인 위도와 경도만을 고려했기 때문이라고 생각할 수 있다. 다른 수많은
잠재변수와 파생변수를 고려한 모델링의 필요성이 드러나는 부분이다. 이를
위경도를 반영한 plot으로 시각화해서 좀 더 세부적으로 확인해보고 모델을
디버깅하는 작업을 Chapter 2에서 진행하겠다.

#### 문제6. KNN 모델이 잘 예측했는지를 확인하기 위해서 아래와 같이 위도와 경도에 대해 원데이터와 예측값을 비교해주는 plot을 그려주세요.

``` r
# 데이터 시각화
ggplot(data = train) +
  geom_point(aes(x = longitude, y = latitude, color = residuals, size = abs(residuals))) +
  scale_color_gradient2(low = "blue", mid = "white", high = "red", name = "res_knn") +
  labs(x = "Longitude", y = "Latitude") +
  guides(color = guide_colorbar(order = 1),
         size = guide_legend(order = 0)) +        # legend들의 순서 설정
  theme_classic()
```

![](%5B시계열자료분석팀%5D-심현구-클린업-3주차-패키지과제_files/figure-gfm/unnamed-chunk-12-1.png)<!-- -->

+@ “위경도 데이터포인트들의 모양이 일본열도와 비슷해서 일본 집들인줄
알았는데 위경도를 검색해보니 중국이네요ㅎㅎ”

위경도가 데이터 중 중앙 지점에 위치하는 집들은 잔차의 절대값이 작다.
그러나 선형으로 좌하향하는 집들, 특히 그래프의 우측 하단의 집들을 보면
동그라미의 크기로 표현되는 잔차의 절대값이 크게 나타나 모델의 예측
정확성을 떨어뜨리는 부분이 해당 부분이라는 것을 확인할 수 있다.

# Chapter2 Modeling Debugging & EDA 2

#### 문제1. 챕터 1에서 강한 상관관계를 가졌던 변수들끼리 PCA를 진행하고 분산이 가장 높은 축의 값을 새로운 파생변수로 만들어주세요.

집 내부시설과 관련된 “totalRooms”, “totalBedrooms”, “population”,
“households” 이 4가지 변수 간의 상관계수가 대부분이 0.9 이상으로 매우
높게 나타났다.

``` r
high_cor_train <- train %>% 
  select("totalRooms", "totalBedrooms", "population", "households")

pca_result1 <- prcomp(high_cor_train, center = TRUE, scale. = TRUE)
highest_var_pc <- pca_result1$rotation[, 1]

pc1 <- as.matrix(high_cor_train) %*% as.matrix(highest_var_pc)

train <- train %>% 
  mutate(pc1 = pc1[,1])
```

#### (보너스 문제 2) pca의 분산을 설명하는 scree plot을 그려주세요.

``` r
variance_explained <- pca_result1$sdev^2

scree_data <- data.frame(PC = 1:length(variance_explained), VarianceExplained = variance_explained)
ggplot(data = scree_data, aes(x = PC, y = VarianceExplained/sum(VarianceExplained))) +
  geom_bar(stat = "identity", fill = "skyblue") +
  geom_text(aes(label = sprintf("%.2f", VarianceExplained/sum(VarianceExplained))), vjust = -0.5) +
  labs(x = "Principal Component (PC)", y = "Variance Explained (%)")
```

![](%5B시계열자료분석팀%5D-심현구-클린업-3주차-패키지과제_files/figure-gfm/unnamed-chunk-14-1.png)<!-- -->

PC1이 전체 분산의 93%를 설명할 정도로 많은 영향을 차지하므로 PC1만으로
차원을 축소해 나타내는 것이 바람직하다. 즉, 4가지 변수를 하나로 축소시켜
표현할 수 있다는 것이다.

#### 문제2. PC1과 어떤 변수들이 어떤 관계를 갖고 있는지 설명해주세요.

``` r
print(pca_result1$rotation[,1])
```

    ##    totalRooms totalBedrooms    population    households 
    ##    -0.4961328    -0.5073099    -0.4866634    -0.5095564

PC를 구성하는 변수들과 모두 음의 관계를 가지고 있으며 그 정도는 모두
-0.5 근처이다.

``` r
# 상관관계를 확인하기 위해 임시로 rich를 수치형으로 변환
train$rich <- as.numeric(train$rich)

cormat_train <- cor(train)

ggcorrplot(cormat_train, lab = T, lab_size = 2, tl.srt = 45, tl.cex = 10, title = "Correlation of CaliforniaHousing")
```

![](%5B시계열자료분석팀%5D-심현구-클린업-3주차-패키지과제_files/figure-gfm/unnamed-chunk-16-1.png)<!-- -->

당연하게도 PC를 구성하는 네 가지 “totalRooms”, “totalBedrooms”,
“population”, “households” 변수들과 상관관계가 매우 높게 나타난다.
다행히도 이외의 변수들과의 상관계수는 낮다. 따라서 모델링 시 네 변수의
정보량은 보존하면서 차원을 축소시킬 수 있는 PC1만을 활용하고 네 변수는
drop 해 주겠다.

``` r
train <- train %>% 
  select(-c("totalRooms", "totalBedrooms", "population", "households"))
```

#### 문제3. res_knn과 pca를 통해 얻은 파생변수, medianIncome, housingMedianAge 간의 관계를 나타내는 plot을 그려주세요.

``` r
df3 <- train %>% 
  select(pc1, medianIncome, housingMedianAge)
df3 <- cbind(residuals, df3)

cormat3 <- cor(df3)

ggcorrplot(cormat3, lab = T, lab_size = 2, tl.srt = 45, tl.cex = 10)
```

![](%5B시계열자료분석팀%5D-심현구-클린업-3주차-패키지과제_files/figure-gfm/unnamed-chunk-18-1.png)<!-- -->
네 변수 간 상관계수가 모두 낮아 선형 관계가 약함을 확인할 수 있다. 특히,
housingMedianAge와 res_knn의 상관계수는 0.03으로, 거의 관계가 없다고 할
수 있다.

#### 문제4. res_knn과 위의 세 변수를 하나의 데이터프레임으로 병합해주세요.

``` r
# 문제3에서 진행 완료.
head(df3, n = 1)
```

    ##   residuals       pc1 medianIncome housingMedianAge
    ## 1 -31771.71 -5831.486       8.3014               21

#### 문제5. 새롭게 만든 데이터프레임을 이용해 res_knn을 예측하는 선형 회귀모델을 학습하고 결과를 확인해주세요.

``` r
linear1 <- lm(residuals ~., data = df3)

summary(linear1)
```

    ## 
    ## Call:
    ## lm(formula = residuals ~ ., data = df3)
    ## 
    ## Residuals:
    ##     Min      1Q  Median      3Q     Max 
    ## -287242  -16777     475   14823  315646 
    ## 
    ## Coefficients:
    ##                    Estimate Std. Error t value Pr(>|t|)    
    ## (Intercept)      -2.234e+04  1.225e+03 -18.238   <2e-16 ***
    ## pc1               2.581e-01  1.672e-01   1.543    0.123    
    ## medianIncome      6.675e+03  1.650e+02  40.445   <2e-16 ***
    ## housingMedianAge -1.477e-01  2.625e+01  -0.006    0.996    
    ## ---
    ## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
    ## 
    ## Residual standard error: 39860 on 16636 degrees of freedom
    ## Multiple R-squared:  0.0906, Adjusted R-squared:  0.09044 
    ## F-statistic: 552.5 on 3 and 16636 DF,  p-value: < 2.2e-16

#### 문제6. Res_knn과 관련 없는 변수인 housingMedianAge를 제외하고 다시 선형 회귀모델을 학습시켜주세요.

``` r
# 상관계수가 0.03으로 절대값이 매우 작고, p-value도 매우 높은 수준으로, res_knn과 관련이 없다고 할 수 있다.

df3 <- df3 %>% 
  select(-housingMedianAge)

linear2 <- lm(residuals ~., data = df3)

summary(linear2)
```

    ## 
    ## Call:
    ## lm(formula = residuals ~ ., data = df3)
    ## 
    ## Residuals:
    ##     Min      1Q  Median      3Q     Max 
    ## -287245  -16775     476   14824  315646 
    ## 
    ## Coefficients:
    ##                Estimate Std. Error t value Pr(>|t|)    
    ## (Intercept)  -2.235e+04  7.736e+02 -28.891   <2e-16 ***
    ## pc1           2.578e-01  1.573e-01   1.639    0.101    
    ## medianIncome  6.675e+03  1.644e+02  40.595   <2e-16 ***
    ## ---
    ## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
    ## 
    ## Residual standard error: 39860 on 16637 degrees of freedom
    ## Multiple R-squared:  0.0906, Adjusted R-squared:  0.09049 
    ## F-statistic: 828.8 on 2 and 16637 DF,  p-value: < 2.2e-16

#### 문제7. 최종결과를 구하기 위해 knn의 예측 값과 위에서 만든 회귀모델을 더해주고 최종결과와 실제 Y의 값의 잔차 간의 residual plot을 그려주세요.

``` r
final_prediction <- y_hat_knn + predict(linear2, df3)
final_residual <- train$medianHouseValue - final_prediction

plot(final_prediction, final_residual, main = "Final Residual Plot",
     xlab = "final_prediction", ylab = "final_residual")
abline(h = 0, col = "red")
```

![](%5B시계열자료분석팀%5D-심현구-클린업-3주차-패키지과제_files/figure-gfm/unnamed-chunk-22-1.png)<!-- -->

#### (보너스 문제 3) 위의 plot을 rich 변수를 통해 확인해보고 더 좋은 예측력을 위해서 어떤 것을 할 수 있을지 말해주세요.

``` r
df_bonus3 <- as.data.frame(cbind(final_prediction, final_residual, train$rich))

# not rich만 따로 저장
df_bonus3_0 <- df_bonus3 %>% 
  filter(df_bonus3$V3==1)


plot(df_bonus3_0$final_prediction, df_bonus3_0$final_residual, main = "Final Residual Plot not Rich",
     xlab = "final_prediction", ylab = "final_residual")
abline(h = 0, col = "red")
```

![](%5B시계열자료분석팀%5D-심현구-클린업-3주차-패키지과제_files/figure-gfm/unnamed-chunk-23-1.png)<!-- -->

``` r
# rich만 따로 저장
df_bonus3_1 <- df_bonus3 %>% 
  filter(df_bonus3$V3==2)

plot(df_bonus3_1$final_prediction, df_bonus3_1$final_residual, main = "Final Residual Plot Rich",
     xlab = "final_prediction", ylab = "final_residual")
abline(h = 0, col = "red")
```

![](%5B시계열자료분석팀%5D-심현구-클린업-3주차-패키지과제_files/figure-gfm/unnamed-chunk-24-1.png)<!-- -->

rich == 0인 경우 예측값이 큰 부분에서 잔차의 분산이 크게 나타났다. 반면
rich == 1인 경우 예측값이 작은 부분에서 잔차의 분산이 크게 나타났다.
따라서 선형회귀모델을 통한 잔차 예측 시 KNN 예측값에 따라 가중치를 줄 수
있을 것이다. 가령 제3사분위수보다 KNN의 예측값이 커 rich == 1로
분류되지만 그 중에서는 값이 작은 경우, 그 정도에 비례하여 잔차를 작게
예측하는 방식으로 최종예측력을 높일 수 있을 것이다.

# Chapter3 Predict

#### 문제1. Train 데이터에 대해 했던 전처리를 test 데이터에서도 반복해주세요.

``` r
high_cor_test <- test %>% 
  select("totalRooms", "totalBedrooms", "population", "households")

pc1 <- as.matrix(high_cor_test) %*% as.matrix(highest_var_pc)
# PC 축 자체는 train data로 구축해 놓은 것을 통해 test data를 선형변환!!!!!!!

test <- test %>% 
  mutate(pc1 = pc1[,1])

test <- test %>% 
  select(-c("totalRooms", "totalBedrooms", "population", "households"))
```

#### 문제2. 학습시킨 KNN 모델과 선형 회귀 모델을 통해서 test 데이터를 예측해주세요!

``` r
# KNN 모델로 학습
pred_test <- predict(final_knn_model, test)             # train으로 학습한 모델로 test 예측
residuals_test <- test$medianHouseValue - pred_test     # residual 계산

# 선형회귀모델로 잔차 예측
df3_test <- test %>% 
  select(pc1, medianIncome)
df3_test <- cbind(residuals_test, df3_test)

linear2 <- lm(residuals_test ~., data = df3_test)

# 합
test_final <- pred_test + predict(linear2, test)
```

#### 문제3. 최종 학습 결과의 RMSE를 구하고 KNN 모델만으로 예측했을 때와 얼만큼의 차이가 있는지 확인해주세요.

``` r
# 최종학습결과의 RMSE
final_rmse <- sqrt(mean((test$medianHouseValue - test_final)^2))
print(final_rmse)
```

    ## [1] 50146.96

``` r
# KNN 모델만으로 예측했을 때의 RMSE
final_knn_rmse <- sqrt(mean((test$medianHouseValue - predict(final_knn_model, test))^2))
print(final_knn_rmse)
```

    ## [1] 53483.1

#### (보너스 문제 4) 만약 선형회귀 모델이 KNN의 잔차에 대해 올바르게 예측을 하지 못한다면 어떤 문제가 생길수 있는지 말해주세요.

-예측 성능 저하: 선형 회귀 모델이 KNN의 잔차를 잘 예측하지 못하면 최종
예측값도 정확하지 않을 가능성이 높다. 따라서 모델의 전체 예측 성능이
저하될 수 있으며, 이는 실제 결과와의 불일치를 초래할 수 있다.

-모델의 불안정성: 잘못된 잔차 예측은 모델의 불안정성을 증가시킬 수 있다.
이는 예측값이 작은 변화에도 민감하게 반응할 수 있음을 의미한다. 따라서
모델의 예측이 불안정하고 신뢰할 수 없게 될 수 있다.

-과적합 가능성: 선형 회귀 모델이 KNN의 잔차를 정확하게 예측하지 못하면
이를 보완하기 위해 더 복잡한 모델을 고려할 수 있다. 그러나 이로 인해
과적합의 위험이 증가할 수 있다. 복잡한 모델은 훈련 데이터에 과도하게
적합되어 새로운 데이터에 대한 일반화 능력이 감소할 수 있다.

-해석의 어려움: 선형 회귀 모델이 잘못된 잔차 예측을 하면 모델의 해석이
어려워진다. 모델의 가중치나 변수 중요도를 해석하는 것이 어려워지며,
모델의 작동 원리를 이해하기 어려울 수 있다.
