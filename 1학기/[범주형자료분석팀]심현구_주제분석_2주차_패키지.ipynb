{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af5328d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c6ff1491",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc3a9a8d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eed69a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Chapter1 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "66c261b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#문제 1. 토큰화 패키지를 사용하여 토큰화를 해봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ac456217",
   "metadata": {},
   "outputs": [],
   "source": [
    "#문제 1-1. 다음 sentence를 문장 단위로 구분하는 문장 토큰화를 실행하세요.(nltk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "00e89fe1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['One of the essential things in the life of a human being is communication.', 'We need to communicate with other human beings to deliver information, express our emotions, present ideas, and much more.', 'The key to communication is language.']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import konlpy\n",
    "\n",
    "sentence1 = 'One of the essential things in the life of a human being is communication. We need to communicate with other human beings to deliver information, express our emotions, present ideas, and much more. The key to communication is language.'\n",
    "result_sent1 = nltk.sent_tokenize(sentence1)\n",
    "print(result_sent1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "23cf2db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#문제 1-2. 다음 sentence를 단어 단위로 토큰화해주세요.(nltk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "29c1c0cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import SpaceTokenizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "sentence2 = 'We need to communicate with other human beings to deliver information, express our emotions, present ideas, and much more.'\n",
    "sentence3 = 'The price\\t of burger \\nin BurgerKing is Rs.36.\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "52abd7ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['We', 'need', 'to', 'communicate', 'with', 'other', 'human', 'beings', 'to', 'deliver', 'information,', 'express', 'our', 'emotions,', 'present', 'ideas,', 'and', 'much', 'more.']\n",
      "['The', 'price\\t', 'of', 'burger', '\\nin', 'BurgerKing', 'is', 'Rs.36.\\n']\n"
     ]
    }
   ],
   "source": [
    "space_tokenizer = SpaceTokenizer()\n",
    "print(space_tokenizer.tokenize(sentence2))\n",
    "\n",
    "space_tokenizer = SpaceTokenizer()\n",
    "print(space_tokenizer.tokenize(sentence3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fd54f6a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SpaceTokenizer는 문장에 들어있는 \\n 등의 제어문자 등을 인식하지 않고 단순히 공백을 기준으로 문장을 토큰화한다.\n",
    "# 또한 마침표 등의 문장부호도 공백 없이 입력되기 때문에 따로 토큰화되지 않는다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a6285c00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['We', 'need', 'to', 'communicate', 'with', 'other', 'human', 'beings', 'to', 'deliver', 'information', ',', 'express', 'our', 'emotions', ',', 'present', 'ideas', ',', 'and', 'much', 'more', '.']\n",
      "['The', 'price', 'of', 'burger', 'in', 'BurgerKing', 'is', 'Rs.36', '.']\n"
     ]
    }
   ],
   "source": [
    "result_sent2 = word_tokenize(sentence2)\n",
    "print(result_sent2)\n",
    "result_sent3 = word_tokenize(sentence3)\n",
    "print(result_sent3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cf7ef649",
   "metadata": {},
   "outputs": [],
   "source": [
    "#반면 word_tokenize는 제어문자나 문장부호를 구분하여 토큰화하기 때문에 가장 많이 사용된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f7fe56f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#문제 1-3. Konlpy(okt) 라이브러리를 사용하여 ‘1-3.txt’ 파일에서 50문장을 골라 한글 형태소 분석을 진행해주세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d5580d5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', 1)\n",
    "from konlpy.tag import Okt\n",
    "\n",
    "txt1_3 = pd.read_csv('C:/Users/mrg07/Desktop/99shimshim/SKKU/PSAT/주제분석/week2/week2_package/1-3.txt', sep = \"\\t\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dacd5e6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "97936     대단한 영화다 보는 내내 한순간도 한눈팔지못하게한 영화다.감히 이 영화는 타인의 추종을 불허하는 내 손가락에 꼽힐정도의 영화다                                                                     \n",
       "29063     볼만은 하다. 하지만 전체적인 스토리로 봤을때 이어지는 내용이 없다. 사람들이 이해할수 있는 내용으로 시작을 해야되는데 이해되는 부분이 많이 없다. 반전이 있어 스릴이 잇었지만 전체적인 내용으로 봐서는 이 영화의 중심이 없는거 같다.         \n",
       "92791     차마 욕은 쓸수 없고..한숨만 나온다. 이 영화를 봤다는게..                                                                                                         \n",
       "90600     파격조차 느껴지지 않는 광신도들                                                                                                                          \n",
       "93980     영화로 부린 마법                                                                                                                                  \n",
       "87680     말도 안되는 축구기술이며, 스토리또한 이해가 잘 되진 않지만... 이렇게 창의력있는 축구 만화는 처음이다                                                                                 \n",
       "47058     많은생각을해주게 하는영화                                                                                                                              \n",
       "24532     유사한 영화: 아임 낫 스케어드                                                                                                                          \n",
       "33703     007 시리즈중 최고라 생각하는데 평점이 왜 이렇게 낮냐 박평식도 8점을 준 영화다                                                                                             \n",
       "83725     딸내미 귀엽고, 단역들 화려하고. 그 뿐.                                                                                                                    \n",
       "34277     톰크루즈...리즈시절....물론 연기력은 호프만이 甲중의 甲                                                                                                          \n",
       "139493    최악                                                                                                                                         \n",
       "17366     히스레저의 팬이라 일부러본건데 내용 자체면과 시대적배경이 신선하고 재밌었다. 뻔한 결말이지만 전체적으로 유쾌하게 승화시킨듯한 영화                                                                   \n",
       "86651     이영화 구려요 존나게 ㅋ                                                                                                                              \n",
       "103424    뭔 개소리지                                                                                                                                     \n",
       "96381     숨죽이면서 봤다. 이런느낌 느끼려고 영화본다.                                                                                                                  \n",
       "99159     정말 재미잇는 리얼 프로그램 다음편이 궁긍해!!                                                                                                                 \n",
       "60202     주제 신선하고 재밌게 봤음 ㅋ                                                                                                                           \n",
       "71178     재밌네,,재밌다.끝까지 지루하지않고 보는 싸구려 7광군지 돈쳐들인 영화들 보다낫네                                                                                              \n",
       "85587     재밌네                                                                                                                                        \n",
       "129025    쓰리데이즈 내용도 출연진도 빵빵하고 정말 재미나게 보고 있어요^^                                                                                                       \n",
       "146672    킹재밌음 진짜 이것만큼 재밌는 뱀파이어 영화는드뭄                                                                                                                \n",
       "29563     속편으로 이어진 인디아나 존스의 B급 아류작 .                                                                                                                 \n",
       "112411    송은채.. 누구도 안 가질 꽃..                                                                                                                         \n",
       "99342     말로 설명이 불가능한 영화                                                                                                                             \n",
       "118524    전쟁에 관한 실사을 가장 사실적으로 알려주는 영화인것 같다.                                                                                                          \n",
       "105437    수준 높은 다큐는 아니지만 맥쿼리리가 얼마나 손쉽게 우리나라사람등의 등골을 빼먹고 있는지는 확실히 알려준다                                                                                \n",
       "16050     미어캣이 너무 신기하고 촬영을 어떻게 했는지도 신기해요.                                                                                                            \n",
       "5585      나만 볼수는 없다. 니들도 봐라. ㅋㅋㅋ 성인이 되어 초딩시절을 되볼아 보며 아직도 미성숙해 있는 주인공들을 보게 된다. 참 무의미하다.                                                               \n",
       "38191     장르부터 다르다 액션,환타지                                                                                                                            \n",
       "107292    진짜 재밌었다 이동욱에 푹 빠지게 해준 드라마.                                                                                                                 \n",
       "5220      어수선하고 뭘 말하고자 하는지 모르겠고, 그냥 감옥에서 일어나는 오바 소동들 오렌지 이즈 더 뉴 블랙하고 비슷하지만, 재미는 반도 못따라감                                                              \n",
       "104660    더이상 숨지 말아야한다.그냥 그대로 지나갈뿐...                                                                                                                \n",
       "69279     밑에 있는분 말씀 공감임                                                                                                                              \n",
       "68614     블록버스터 사극의 시작, 조선왕조를 다루지 않은 첫 사극, 전무후무한 200부작의 사극이면서도 막장으로 흐르지 않고 끝까지 시청률 40프로대 유지. 완벽한 스토리와 자주적이고 웅장한 역사를 보여준 그야말로 레전드인 최고의 사극             \n",
       "120754    최악의영화별반쪽주기도아깝다전무후무내겐0점짜리영화한마디로보는순간뭐밟은느낌                                                                                                    \n",
       "42018     개인적으로 철학이고 메시지 담긴 영화 좋아하는데, 이건 너무 지루하다. 인간에게 응원을 줄 때 흑백에서 컬러로 바뀌는데. 이 효과를 위해 많은 시간을 흑백로 봐야한다. 굉장히 경제적이지 못한 연출효과다. 그리고 전개가 너무 루즈하다.         \n",
       "102125    점점 삼천포로 흘러가는듯.. 승희가 태범이랑 결혼 할 때 부터 이상해지더니 방향 잃은 배처럼 이야기가 표류하고 있네요 재미있게 보고 있었는데 영~실망입니다                                                     \n",
       "3145      반전이 꽤나 신선했다.                                                                                                                               \n",
       "3176      모든 사람들 웃기만 했다...                                                                                                                           \n",
       "126891    미루고있다면 지금 당장 봐야 할 영화명작..                                                                                                                   \n",
       "31674     극한상황에서 인간이 얼마나 나약해지는가...그리고 얼마나 악랄해지는가...구원을 원한 사람은 죽고. 스스로 구원한 자는 살았다... 희망을 찾아 떠난 자들은 스스로 구원하지 못했기에 절망을 겪은것이고. 스스로 구원한 여자는 희망을 스스로 찾은것이다.\n",
       "77707     왜안 저스티스리그요                                                                                                                                 \n",
       "133930    뭐라 할 말이 없는 영화ㅋ                                                                                                                             \n",
       "44084     사랑 하기전엔 이 영화를 먼저 보기.잔잔했지만 꽤 파워있었다                                                                                                          \n",
       "42974     올가미의 후광은 더이상 없다                                                                                                                            \n",
       "110769    소재도 참신했고 무엇보다 진짜 천사같은 엠마누엘 베아르의 미모는 잊을수 없다                                                                                                 \n",
       "48409     이건 아닙니다. 절대로 보지마세요 웬만하면..                                                                                                                  \n",
       "74197     호평만겁나게 많어 ㅋㅋ 칭찬게시판인가                                                                                                                       \n",
       "91755     한심한 수준의 영화지만, 스포츠카 보는 재미로.                                                                                                                 \n",
       "Name: document, dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt1_3 = txt1_3['document'].sample(50) #댓글 물타기로 인한 편향을 방지하기 위해 random한 50개를 가져옴.\n",
    "txt1_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2f2066b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['대단하다', '영화', '보다', '내내', '한순간', '한눈팔다', '하다', '영화', '감히', '이', '영화', '타인', '추종', '불허', '하다', '내', '손가락', '꼽히다', '정도', '영화']\n",
      "['볼', '하다', '하지만', '전체', '적', '스토리', '보다', '때', '이어지다', '내용', '없다', '사람', '들', '이해', '하다', '있다', '내용', '시작', '하다', '되다', '이해', '되다', '부분', '많이', '없다', '반전', '있다', '스릴', '잇다', '전체', '적', '내용', '보다', '서다', '이', '영화', '중심', '없다', '같다']\n",
      "['차마', '욕', '쓸다', '없다', '한숨', '나오다', '이', '영화', '보다']\n",
      "['파격', '느껴지다', '않다', '광신도', '들']\n",
      "['영화로', '부리다', '마법']\n",
      "['말', '안되다', '축구', '기술', '스토리', '또한', '이해', '자다', '되다', '않다', '이렇게', '창의력', '있다', '축구', '만화', '처음']\n",
      "['많다', '생각', '해주다', '하다', '영화']\n",
      "['유사하다', '영화', '아', '임', '낫', '스', '케어드']\n",
      "['007', '시리즈', '중', '최고', '생각', '하다', '평점', '왜', '이렇게', '낮다', '박평', '식', '8', '점', '준', '영화']\n",
      "['딸내미', '귀엽다', '단역', '들', '화려하다', '그', '뿐']\n",
      "['톰크루즈', '리즈시절', '물론', '연기력', '호프', '甲', '중', '甲']\n",
      "['최악']\n",
      "['히스', '레저', '팬', '일부러', '보다', '내용', '자체', '과', '시대', '적', '배경', '신선하다', '재밌다', '뻔하다', '결말', '전체', '적', '유쾌하다', '승화', '시키다', '한', '영화']\n",
      "['이영화', '구려', '존나', 'ㅋ']\n",
      "['뭔', '개', '소리']\n",
      "['숨', '죽이다', '보다', '이런', '느낌', '느끼다', '영화', '보다']\n",
      "['정말', '재미', '잇다', '리얼', '프로그램', '다음', '편이', '궁긍해']\n",
      "['주제', '신선하다', '재밌다', '보다', 'ㅋ']\n",
      "['재밌다', '재밌다', '끝', '지루하다', '않다', '보다', '싸구려', '7', '광구', '돈', '치다', '들이다', '영화', '들', '보다', '낫']\n",
      "['재밌다']\n",
      "['쓰리데이즈', '내용', '출연', '진도', '빵빵', '정말', '재미나', '보고', '있다']\n",
      "['킹', '재밌다', '진짜', '이', '것', '재밌다', '뱀파이어', '영화', '드뭄']\n",
      "['속편', '이어진', '인디아나', '존스', 'B', '급', '아', '류작']\n",
      "['송은채', '누구', '안', '가지다', '꽃']\n",
      "['말로', '설명', '불가능하다', '영화']\n",
      "['전쟁', '관', '실', '사다', '가장', '사실', '적', '알다', '영화인', '것', '같다']\n",
      "['수준', '높다', '다큐', '아니다', '맥', '쿼리', '리가', '얼마나', '손쉽다', '우리나라', '사람', '등', '등골', '빼먹다', '있다', '확실하다', '알다', '주다']\n",
      "['미어캣', '너무', '신기하다', '촬영', '어떻다', '하다', '신기하다']\n",
      "['나', '보다', '없다', '니', '들', '보다', 'ㅋㅋㅋ', '성인', '되어다', '초딩', '시절', '되다', '볼', '보다', '아직도', '밉다', '성숙하다', '있다', '주인공', '들', '보다', '되다', '차다', '무의미하다']\n",
      "['장르', '다르다', '액션', '환타지']\n",
      "['진짜', '재밌다', '이동욱', '푹', '빠지다', '해주다', '드라마']\n",
      "['어수선하다', '뭘', '말', '하다', '하다', '모르다', '그냥', '감옥', '일어나다', '오', '바', '소동', '들', '오렌지', '이즈', '더', '뉴', '블랙', '비슷하다', '재미', '반도', '못', '따라가다']\n",
      "['더', '이상', '숨다', '말', '야하다', '그냥', '그대로', '지나가다', '뿐']\n",
      "['밑', '있다', '분', '말씀', '공', '감임']\n",
      "['블록버스터', '사극', '시작', '조선왕조', '다루다', '않다', '첫', '사극', '전무후무', '200', '부작', '사극', '이', '서도', '막장', '흐르다', '않다', '끝', '시청률', '40', '프로', '대', '유지', '완벽하다', '스토리', '자주', '적', '웅장', '역사', '보여주다', '그야말로', '레전드', '최고', '사극']\n",
      "['최악', '영화', '별', '반쪽', '주기도', '아깝다', '전무후무', '내', '겐', '0', '점', '짜다', '영화', '한마디', '보다', '순간', '뭐', '밟다', '느낌']\n",
      "['개인', '적', '철학', '메시지', '담기다', '영화', '좋아하다', '이', '것', '너무', '지루하다', '인간', '응원', '줄', '때', '흑백', '컬러', '바뀌다', '이', '효과', '위해', '많다', '시간', '흑백', '로', '보다', '야하다', '굉장하다', '경제', '적', '못', '연출', '효과', '그리고', '전개', '너무', '루즈', '하다']\n",
      "['점점', '삼천포', '흘러가다', '승희', '태범', '결혼', '하다', '때', '부터', '이상하다', '방향', '잃다', '배', '이야기', '표류', '있다', '재미있다', '보고', '있다', '영', '실망', '이다']\n",
      "['반전', '꽤', '신선하다']\n",
      "['모든', '사람', '들', '웃기', '하다']\n",
      "['미루다', '지금', '당장', '보다', '하다', '영화', '명작']\n",
      "['극', '한', '상황', '인간', '얼마나', '나약하다', '그리고', '얼마나', '악랄하다', '구원', '원한', '사람', '죽다', '스스로', '구원', '자다', '살다', '희망', '찾다', '떠나다', '자', '들', '스스로', '구원', '하다', '못', '하다', '절망', '겪다', '스스로', '구원', '여자', '희망', '스스로', '찾다']\n",
      "['왜안', '저스티스리그']\n",
      "['뭐라다', '하다', '말', '없다', '영화', 'ㅋ']\n",
      "['사랑', '하다', '이', '영화', '먼저', '보기', '잔잔하다', '꽤', '파워', '있다']\n",
      "['올가미', '후광', '더', '이상', '없다']\n",
      "['소재', '참신하다', '무엇', '진짜', '천사', '같다', '엠마누엘', '베', '아르', '미모', '잊다', '없다']\n",
      "['이', '것', '아니다', '절대로', '보지', '말다', '웬만하다']\n",
      "['호평', '겁나다', '많다', 'ㅋㅋ', '칭찬', '게시판']\n",
      "['한심하다', '수준', '영화', '스포츠카', '보다', '재미']\n"
     ]
    }
   ],
   "source": [
    "for i, document in enumerate(txt1_3):\n",
    "    okt = Okt()\n",
    "    clean_words = []\n",
    "    for word in okt.pos(document, norm=True, stem=True): #어간 추출\n",
    "        if word[1] not in ['Josa', 'Eomi', 'Punctuation']: #조사, 어미, 구두점 제외 \n",
    "            clean_words.append(word[0])\n",
    "    print(clean_words) #['스토리', '진짜', '너무', '노잼']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "278a80cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce0eb1e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#문제 2. nltk 라이브러리를 사용하여 토큰화 후 다음 문장에서 불용어 제거를 해보세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0628e291",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize \n",
    "from konlpy.tag import Okt\n",
    "\n",
    "sent2 = \"We need a common language to communicate, which both ends of the conversation can understand.\"\n",
    "\n",
    "stop_words_list = stopwords.words('english')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "word_tokens = word_tokenize(sent2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "97d87341",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "불용어 제거 전 : ['We', 'need', 'a', 'common', 'language', 'to', 'communicate', ',', 'which', 'both', 'ends', 'of', 'the', 'conversation', 'can', 'understand', '.']\n",
      "불용어 개수 : 179\n",
      "불용어 제거 후 : ['We', 'need', 'common', 'language', 'communicate', ',', 'ends', 'conversation', 'understand', '.']\n",
      "불용어 10개 출력 : ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\"]\n"
     ]
    }
   ],
   "source": [
    "result = []\n",
    "\n",
    "for word in word_tokens: \n",
    "    if word not in stop_words: \n",
    "        result.append(word) \n",
    "\n",
    "print('불용어 제거 전 :',word_tokens) \n",
    "print('불용어 개수 :', len(stop_words_list))\n",
    "print('불용어 제거 후 :',result)\n",
    "print('불용어 10개 출력 :',stop_words_list[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e58b4a2e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d26b663a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#문제 3. nltk 라이브러리에서는 어간 추출 알고리즘 중 하나인 포터 알고리즘을 제공합니다.\n",
    "#여러 단어들을 넣어보며 원형을 잘 보존하는지 확인해보세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "62a45bb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "play play play\n",
      "laugh laugh laugh\n",
      "happier happiest\n",
      "go goe gone went\n",
      "eat ate eaten\n",
      "have had\n"
     ]
    }
   ],
   "source": [
    "from nltk import PorterStemmer, word_tokenize\n",
    "\n",
    "porter = PorterStemmer()\n",
    "\n",
    "print(porter.stem('plays'),porter.stem('played'),porter.stem('playing'))\n",
    "print(porter.stem('laughs'),porter.stem('laughed'),porter.stem('laughing'))\n",
    "print(porter.stem('happier'),porter.stem('happiest'))\n",
    "\n",
    "print(porter.stem('going'), porter.stem('goes'), porter.stem('gone'), porter.stem('went'))\n",
    "print(porter.stem('eat'), porter.stem('ate'), porter.stem('eaten'))\n",
    "print(porter.stem('have'), porter.stem('had'))\n",
    "\n",
    "#불규칙 활용 동사는 3단현, 과거형으로부터 원형 추출이 안 됨\n",
    "#속도가 빠르며 차원축소에 용이함: 단어가 다양한 형태여도 원형에 대해서만 임배딩을 해주면 되니까."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "0123dead",
   "metadata": {},
   "outputs": [],
   "source": [
    "#문제 4. nltk 라이브러리에는 표제어 추출 알고리즘을 제공합니다. 여러 단어들을 넣어보며 표제어를 잘 추출하는지 확인해보세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "3f68e712",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fly\n",
      "fly\n",
      "fly\n",
      "fly\n",
      "flying\n"
     ]
    }
   ],
   "source": [
    "from nltk import PorterStemmer, WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "print(lemmatizer.lemmatize('flying', pos='v'))    # pos: 품사     v: verb   n: noun\n",
    "\n",
    "print(lemmatizer.lemmatize('flies', pos='n'))    # 파리의 단수형\n",
    "print(lemmatizer.lemmatize('flies', pos='v'))    # 날다의 동사원형\n",
    "\n",
    "print(lemmatizer.lemmatize('flying', pos='v'))   # 날다의 동사원형\n",
    "print(lemmatizer.lemmatize('flying', pos='n'))   # 동명사 '나는 것' 보존\n",
    "\n",
    "#속도가 느리지만 품사를 보존하며 추출할 수 있음.\n",
    "#같은 표현이라도 품사가 다른 것을 반영"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e20377",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b407b917",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Chapter 2 임베딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6b64a63a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#문제1. 횟수 기반 임베딩 중 하나인 CountVectorizer를 다음 코퍼스에 사용하여 얻은 결과를 배열로 변환해주세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0c176657",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import konlpy\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "941e9237",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = ['This is the first document.', 'This is the second second document.', 'And the third one.', 'Is this the first document?', 'The last document?']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "6ac5a3b6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 1 1 0 0 0 1 0 1]\n",
      " [0 1 0 1 0 0 2 1 0 1]\n",
      " [1 0 0 0 0 1 0 1 1 0]\n",
      " [0 1 1 1 0 0 0 1 0 1]\n",
      " [0 1 0 0 1 0 0 1 0 0]]\n"
     ]
    }
   ],
   "source": [
    "transformer = CountVectorizer()\n",
    "transformer.fit(corpus)\n",
    "corpus = transformer.transform(corpus)\n",
    "print(corpus.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "df787e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#문제2. 예측 기반 임베딩인 Word2Vec를 사용하여 단어간 의미 유사도를 구해보겠습니다.\n",
    "#문제2-1. ‘2-1.txt’ 파일을 불러와주세요. 파일은 소설 피터팬 원문의 일부분입니다.\n",
    "#문제2-2. 파일 문장을 단어로 모두 토큰화해주세요.\n",
    "\n",
    "#문제2-3. 토큰화된 단어를 모두 소문자로 변환하여 저장해주세요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "ead3cfc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Once', 'upon', 'a', 'time', 'in', 'London,', 'the', 'darlings', 'went', 'out', 'to', 'a', 'dinner', 'party', 'leaving', 'their', 'three', 'children', 'Wendy,', 'Jhon,', 'and', 'Michael', 'at', 'home.']\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'int' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[70], line 19\u001b[0m\n\u001b[0;32m     17\u001b[0m token2_lower \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m token2 :\n\u001b[1;32m---> 19\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(i):\n\u001b[0;32m     20\u001b[0m         token2_lower\u001b[38;5;241m.\u001b[39mappend(token2[i][j]\u001b[38;5;241m.\u001b[39mlower())     \u001b[38;5;66;03m#소문자로 변환\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28mprint\u001b[39m(token2_lower)\n",
      "\u001b[1;31mTypeError\u001b[0m: 'int' object is not iterable"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "import re\n",
    "\n",
    "f = open('C:/Users/mrg07/Desktop/99shimshim/SKKU/PSAT/주제분석/week2/week2_package/2-1.txt', encoding='utf-8')\n",
    "txt2_1 = f.read()\n",
    "txt2_1\n",
    "\n",
    "token2_sent = nltk.sent_tokenize(txt2_1)\n",
    "token2 = re.sub(r'[^a-zA-Z\\. ]', ' ', txt2_1)\n",
    "token2 = token2.split('. ')\n",
    "token2 = [s.split() for s in token2_sent]\n",
    "#print(token2)\n",
    "\n",
    "print(token2[0])\n",
    "\n",
    "token2_lower = []\n",
    "for i in token2 :\n",
    "    for j in range(0, len(i)):\n",
    "        token2_lower.append(token2[i][j].lower())     #소문자로 변환\n",
    "\n",
    "print(token2_lower)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "fe2a66b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#문제2-4. CBOW 방식으로 벡터화를 진행한 후 ‘peter’와 ‘wendy’, ‘peter’와 ‘hook’의 유사도를 각각 구해주세요. (min_count = 1, vector_size = 100, window=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "cb6e5666",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import zipfile\n",
    "from lxml import etree\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d835c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "b6ddbfd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총 샘플의 개수 : 89\n"
     ]
    }
   ],
   "source": [
    "print('총 샘플의 개수 : {}'.format(len(token2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c019e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://wikidocs.net/50739"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a2d4de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#http://doc.mindscale.kr/km/unstructured/11.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3324a7f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "784a1f56",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "4239c5a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "model = Word2Vec(sentences = token2, vector_size=100, window=5, min_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "667cc0eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "da85dc3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0064410693"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.similarity('Peter', 'Wendy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e1237d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.similarity('Peter', 'Hook')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc0e4fa8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e264d78e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#문제2-5. 문제 ‘2-4’를 skip-gram 방식으로 진행해주세요. 설정값도 동일합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "81e5c448",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(sentences = token2, vector_size=100, window=5, min_count=1, sg=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "557dae62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.12741406"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.similarity('Peter', 'Wendy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "9dd86418",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.12661389"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.similarity('Peter', 'Hook')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb0c297d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#문제2-6. Word2Vec의 단점을 보완한 ‘FastText’ 알고리즘을 사용하여 문제 ‘2-4’를 해결해주세요.\n",
    "#그리고 온라인 텍스트와 같이 오탈자가 많은 경우에 ‘FastText’가 왜 더욱 잘 작동할지 특징을 간략히만 적어주세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae53d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Word2Vec는 단어를 쪼개질 수 없는 단위로 생각한다면, FastText는 하나의 단어 안에도 여러 단어들이 존재하는 것으로 간주합니다. 내부 단어. 즉, 서브워드(subword)를 고려하여 학습합니다.\n",
    "#Word2Vec는 학습 데이터에 존재하지 않는 단어, 즉, 모르는 단어에 대해서는 임베딩 벡터가 존재하지 않기 때문에 단어의 유사도를 계산할 수 없습니다.Word2Vec의 경우에는 등장 빈도 수가 적은 단어(rare word)에 대해서는 임베딩의 정확도가 높지 않다는 단점이 있었습니다. 참고할 수 있는 경우의 수가 적다보니 정확하게 임베딩이 되지 않는 경우입니다.\n",
    "#FastText에서는 각 단어는 글자 단위 n-gram의 구성으로 취급합니다. n을 몇으로 결정하는지에 따라서 단어들이 얼마나 분리되는지 결정됩니다.\n",
    "#이를 이용해서 모르는 단어(Out Of Vocabulary, OOV)에 대한 대응이 가능하고, 단어 집합 내 빈도 수가 적었던 단어(Rare Word)에 대한 대응이 가능해집니다.\n",
    "\n",
    "#출처: https://wikidocs.net/22883"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6983ce19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a36fd4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#문제3. attention은 nlp 분야에서 매우 중요한 매커니즘인데, 이를 사용한 모델 중 대표적인 BERT 모델이 어떻게 토큰화부터 임베딩까지 진행하여 높은 성능을 보여주었는지 알아보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b5949e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#문제3-1. 다음 문장을 사전훈련된 bert tokenizer를 이용하여 토큰화해주세요. ‘bert-base-uncased’로 설정하여 기본 모델을 사용하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "06c9276f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 21239, 2101, 1110, 1126, 24568, 4841, 2427, 1104, 23213, 117, 2775, 2598, 117, 1105, 8246, 4810, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "text3 = 'NLP is an interdisciplinary subfield of linguistics, computer science, and artificial intelligence'\n",
    "\n",
    "tokenizer(text3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "38da5370",
   "metadata": {},
   "outputs": [],
   "source": [
    "#문제3-2. Bert는 토큰을 이용하여 문장을 구분하는데 문장 앞에는 ‘[CLS]’, 뒤에는 ‘[SEP]’ 토큰을 붙입니다.\n",
    "#위의 ‘text’ 또한 두 개의 토큰을 앞뒤에 붙여 저장해주세요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8ccb56c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "text3 = '[CLS] NLP is an interdisciplinary subfield of linguistics, computer science, and artificial intelligence [SEP]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "532b16a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#문제3-3. Bert는 tokenize() 함수를 사용하여 토큰화를 진행한 뒤 결과를 확인해주세요. 단어 단위로 토큰화를 한 후 알 수 없는 단어는 더 작은 단위로 분리한 것을 확인할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6d8b1179",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'NL', '##P', 'is', 'an', 'interdisciplinary', 'sub', '##field', 'of', 'linguistics', ',', 'computer', 'science', ',', 'and', 'artificial', 'intelligence', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "tokenized_text3 = tokenizer.tokenize(text3)\n",
    "print(tokenized_text3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9a427c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "#문제3-4. convert_tokens_to_ids() 함수를 사용하여 각 토큰의 인덱스를 알 수 있습니다. 결과값이 위의‘input_ids’인 것을 알 수 있습니다. 결과를 따로 저장해주세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "422252ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = tokenizer.convert_tokens_to_ids(tokenized_text3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d4f2945e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#문제3-5. 문제에서 하나의 문장을 사용 중이기 때문에 3가지 인풋 모두 사용하지 않아도 됩니다.‘attention_mask’와 같은 리스트만 만들주세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "704ce845",
   "metadata": {},
   "outputs": [],
   "source": [
    "attm = tokenizer(text3)['attention_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1a0ce01e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#문제3-6. 문제 4-4~5에서 만든 두 가지 리스트를 텐서로 변환해주세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7c958f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f3cc06db",
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = torch.tensor(ids)\n",
    "attm = torch.tensor(attm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "914ca8f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#문제3-7. BertModel(bert-base-uncased, output_hidden_states = True로 설정)를 구축한 후 텐서를 넣어 결과를 확인해주세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1fcb4f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertConfig, BertModel\n",
    "\n",
    "config = BertConfig()\n",
    "model = BertModel(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb3e4c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://wikidocs.net/166799"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2e5a7154",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = BertModel.from_pretrained(\"bert-base-uncased\", output_hidden_states = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fc5e39b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7425f195",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 2, got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[40], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43masdf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:974\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    971\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    972\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou have to specify either input_ids or inputs_embeds\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 974\u001b[0m batch_size, seq_length \u001b[38;5;241m=\u001b[39m input_shape\n\u001b[0;32m    975\u001b[0m device \u001b[38;5;241m=\u001b[39m input_ids\u001b[38;5;241m.\u001b[39mdevice \u001b[38;5;28;01mif\u001b[39;00m input_ids \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m inputs_embeds\u001b[38;5;241m.\u001b[39mdevice\n\u001b[0;32m    977\u001b[0m \u001b[38;5;66;03m# past_key_values_length\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: not enough values to unpack (expected 2, got 1)"
     ]
    }
   ],
   "source": [
    "model(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47544cd3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79cc537a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f814a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c65f59c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d020c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Chapter 3 정형데이터 적용(LSTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7802f371",
   "metadata": {},
   "outputs": [],
   "source": [
    "#문제1. ‘Modelling.csv’ 데이터를 불러온 후 구조를 파악해주세요. 이는 기업의 주가 정보 데이터입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d8114a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "data3 = pd.read_csv('C:/Users/mrg07/Desktop/99shimshim/SKKU/PSAT/주제분석/week2/week2_package/Modelling.csv')\n",
    "#data3.info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "310e65f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#문제2. ‘Date’ 칼럼을 datetime 형식으로 바꾼 후 ‘Date’ 칼럼을 인덱스로 설정해주세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "66b91170",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import warnings\n",
    "\n",
    "datetime.strptime(data3['Date'][1], '%Y-%m-%d')\n",
    "\n",
    "for i in range(len(data3)):\n",
    "    data3['Date'][i] = datetime.strptime(data3['Date'][i], '%Y-%m-%d')\n",
    "\n",
    "data3 = data3.set_index(keys='Date')\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "705ae6eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#문제3. ‘Volume’ 칼럼을 실수로 변환한 후. ‘Volume’ 칼럼을 레이블(y), 나머지를 훈련(x) 데이터로 분할해주세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "19666582",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data3['Volume'] = data3['Volume'].astype(float)\n",
    "y = pd.DataFrame(data3['Volume'])\n",
    "x = pd.DataFrame(data= data3.iloc[:,0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "53654530",
   "metadata": {},
   "outputs": [],
   "source": [
    "#문제4. 데이터 간의 분포를 맞추기 위해 스케일링을 해보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "da489b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#문제4-1. StandardScaler를 사용하여 x 데이터를, MinMaxScaler()를 사용하여 y 데이터를 스케일링해주세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9ab55d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "std_scaler = StandardScaler()\n",
    "std_scaled = std_scaler.fit_transform(x)\n",
    "\n",
    "mm_scaler = MinMaxScaler()\n",
    "mm_scaler.fit(y)\n",
    "mm_scaled = mm_scaler.transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d6984f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "#문제5. x와 y데이터는 253행을 갖고 있는데 그 중 200개는 훈련 데이터셋, 나머지는 테스트 데이터셋으로 각각 분할해주세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f482d5d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200 53\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "np.random.seed(2017)\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(x, y, train_size=200)\n",
    "\n",
    "print(len(X_train), len(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "94b360e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#문제6. 데이터를 모두 텐서로 변환한 후 x 데이터를 LSTM의 입력 형태와 맞추기 위해 형태를 변경해주세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d722cdbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#문제6-1. Train x 데이터는 [200,1,5], test x 데이터는 [53,1,5]의 형태를 가져야 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d137c7b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset # 텐서데이터셋"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "254358ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.values\n",
    "X_test = X_test.values\n",
    "\n",
    "X_train_tensor = torch.FloatTensor(X_train)\n",
    "X_test_tensor = torch.FloatTensor(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "9e2bc541",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([200, 5])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff866c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#문제7. LSTM 모델을 class의 형태로 만들어 구축해주세요. 모델을 만들 때에는 torch.nn.Module을 상속 받아야하며, __init__, forward 함수가 필수적으로 구현되어 있어야 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "72586ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "517acb1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, layer_dim, output_dim):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        # Hidden dimensions\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        # Number of hidden layers\n",
    "        self.layer_dim = layer_dim\n",
    "\n",
    "        # Building your LSTM\n",
    "        # batch_first=True causes input/output tensors to be of shape\n",
    "        # (batch_dim, seq_dim, feature_dim)\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, layer_dim, batch_first=True)\n",
    "\n",
    "        # Readout layer\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Initialize hidden state with zeros\n",
    "        h0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).requires_grad_()\n",
    "\n",
    "        # Initialize cell state\n",
    "        c0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).requires_grad_()\n",
    "\n",
    " \n",
    "        # We need to detach as we are doing truncated backpropagation through time (BPTT)\n",
    "        # If we don't, we'll backprop all the way to the start even after going through another batch\n",
    "        out, (hn, cn) = self.lstm(x, (h0.detach(), c0.detach()))\n",
    "\n",
    "        # Index hidden state of last time step\n",
    "        # out.size() --> 100, 28, 100\n",
    "        # out[:, -1, :] --> 100, 100 --> just want last time step hidden states! \n",
    "        out = self.fc(out[:, -1, :]) \n",
    "        # out.size() --> 100, 10\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a564bf3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 200\n",
    "hidden_dim = 2\n",
    "layer_dim = 1\n",
    "output_dim = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dcb6767",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.deeplearningwizard.com/deep_learning/practical_pytorch/pytorch_lstm_neuralnetwork/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f5930cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3005179",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be29d24",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c05f129",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ccf4b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fd8a229",
   "metadata": {},
   "outputs": [],
   "source": [
    "죄송합니다... 바닥난 체력에 예비군을 들이부었더니 약간의 병을 얻어서... 다음주엔 건강히 해내겠습니다!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
